{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "322290e0",
   "metadata": {},
   "source": [
    "# Split between languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "be229a88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import os \n",
    "from sklearn.model_selection import train_test_split\n",
    "import json\n",
    "\n",
    "_LANG_NAME={\n",
    "    \"af\": \"Afrikaans\",\n",
    "\t\"ar\": \"Arabic\",\n",
    "\t\"az\": \"Azerbaijani\",\n",
    "\t\"bn\": \"Bengali\",\n",
    "\t\"cs\": \"Czech\",\n",
    "\t\"de\": \"German\",\n",
    "\t\"en\": \"English\",\n",
    "\t\"es\": \"Spanish\",\n",
    "\t\"et\": \"Estonian\",\n",
    "\t\"fa\": \"Persian\",\n",
    "\t\"fi\": \"Finnish\",\n",
    "\t\"fr\": \"French\",\n",
    "\t\"gl\": \"Galician\",\n",
    "\t\"gu\": \"Gujarati\",\n",
    "\t\"he\": \"Hebrew\",\n",
    "\t\"hi\": \"Hindi\",\n",
    "\t\"hr\": \"Croatian\",\n",
    "\t\"id\": \"Indonesian\",\n",
    "\t\"it\": \"Italian\",\n",
    "\t\"ja\": \"Japanese\",\n",
    "\t\"ka\": \"Georgian\",\n",
    "\t\"kk\": \"Kazakh\",\n",
    "\t\"km\": \"Khmer\",\n",
    "\t\"ko\": \"Korean\",\n",
    "\t\"lt\": \"Lithuanian\",\n",
    "\t\"lv\": \"Latvian\",\n",
    "\t\"mk\": \"Macedonian\",\n",
    "\t\"ml\": \"Malayalam\",\n",
    "\t\"mn\": \"Mongolian\",\n",
    "\t\"mr\": \"Marathi\",\n",
    "\t\"my\": \"Burmese\",\n",
    "\t\"ne\": \"Nepali\",\n",
    "\t\"nl\": \"Dutch\",\n",
    "\t\"pl\": \"Polish\",\n",
    "\t\"ps\": \"Pashto\",\n",
    "\t\"pt\": \"Portuguese\",\n",
    "\t\"ro\": \"Romanian\",\n",
    "\t\"ru\": \"Russian\",\n",
    "\t\"si\": \"Sinhala\",\n",
    "\t\"sl\": \"Slovene\",\n",
    "\t\"sv\": \"Swedish\",\n",
    "\t\"sw\": \"Swahili\",\n",
    "\t\"ta\": \"Tamil\",\n",
    "\t\"te\": \"Telugu\",\n",
    "\t\"th\": \"Thai\",\n",
    "\t\"tl\": \"Tagalog\",\n",
    "\t\"tr\": \"Turkish\",\n",
    "\t\"uk\": \"Ukrainian\",\n",
    "\t\"ur\": \"Urdu\",\n",
    "\t\"vi\": \"Vietnamese\",\n",
    "\t\"xh\": \"Xhosa\",\n",
    "\t\"zh\": \"Chinese\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "69147b11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/en/train.json\n",
      "data/zh/train.json\n",
      "data/zh/test.json\n",
      "data/en/test.json\n",
      "data/en/train_cla.json\n",
      "data/zh/train_cla.json\n"
     ]
    }
   ],
   "source": [
    "languages = [\"en\",\"zh\"]\n",
    "\n",
    "def read_files_and_split(languages):\n",
    "    src_lang_path = languages[0]+\"_formats.json\"\n",
    "    src_lang_path = os.path.join(\"templates\", src_lang_path)\n",
    "    src_json = pd.read_json(src_lang_path)\n",
    "    tgt_lang_path = languages[1]+\"_formats.json\"\n",
    "    tgt_lang_path = os.path.join(\"templates\", tgt_lang_path)\n",
    "    tgt_json = pd.read_json(tgt_lang_path)\n",
    "\n",
    "    path = os.path.join(\"data\", \"people.csv\")\n",
    "    src_lang_train = pd.read_csv(path)\n",
    "\n",
    "    tgt_lang_train, tgt_lang_test = train_test_split(src_lang_train, test_size=0.5, random_state=42)\n",
    "\n",
    "    return src_json, tgt_json, src_lang_train, tgt_lang_train, tgt_lang_test\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def fill_the_templates(dataframe, template_json, language, typ, for_CLA_format=False):\n",
    "    dir = os.path.join(\"data\", language)\n",
    "    os.makedirs(dir, exist_ok=True)\n",
    "    if for_CLA_format:\n",
    "        path = os.path.join(dir, f\"{typ}_cla.json\")\n",
    "    else:\n",
    "        path = os.path.join(dir, f\"{typ}.json\") \n",
    "\n",
    "    print(path)\n",
    "    all_data = []\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as outfile:\n",
    "        for person in dataframe.iterrows():\n",
    "            for key in template_json.keys():\n",
    "                for value in template_json[key]:\n",
    "                    dictr = {}\n",
    "                    question = value[\"question\"]\n",
    "                    answer = value[\"answer\"]\n",
    "                    question = question.replace(\"{name}\", person[1][\"name\"])\n",
    "                    answer = answer.replace(\"{name}\", person[1][\"name\"])\n",
    "                    if key ==\"Place of living\":\n",
    "                        answer = answer.replace(\"{location}\", person[1][\"city\"])\n",
    "                    if key==\"Birth\":\n",
    "                        answer = answer.replace(\"{date}\", str(person[1][\"birth_date\"]))\n",
    "                    if key==\"Death\":\n",
    "                        answer = answer.replace(\"{date}\", str(person[1][\"death_date\"]))\n",
    "                    if for_CLA_format:\n",
    "                        clm_text = question + \" Answer in \" + _LANG_NAME[language] + \". \" + answer\n",
    "                        dictr = {   \n",
    "                            \"sent0\": \"\",\n",
    "                            \"sent1\": \"\",\n",
    "                            \"clm_text\": clm_text,\n",
    "                            \"clm_prompt_len\": len(clm_text) - len(answer),\n",
    "                        }\n",
    "                        \n",
    "\n",
    "                    else:\n",
    "                        dictr[\"prompt\"] = question\n",
    "                        dictr[\"answer\"] = answer\n",
    "                    all_data.append(dictr)\n",
    "        json.dump(all_data, outfile, ensure_ascii=False, indent=2)\n",
    "                    \n",
    "\n",
    "src_json, tgt_json, src_lang_train, tgt_lang_train, tgt_lang_test = read_files_and_split(languages)\n",
    "fill_the_templates(src_lang_train, src_json, languages[0], \"train\", for_CLA_format=False)\n",
    "fill_the_templates(tgt_lang_train, tgt_json, languages[1], \"train\", for_CLA_format=False)\n",
    "fill_the_templates(tgt_lang_test, tgt_json, languages[1], \"test\", for_CLA_format=False)\n",
    "fill_the_templates(tgt_lang_test, src_json, languages[0], \"test\", for_CLA_format=False)\n",
    "\n",
    "fill_the_templates(src_lang_train, src_json, languages[0], \"train\", for_CLA_format=True)\n",
    "fill_the_templates(tgt_lang_train, tgt_json, languages[1], \"train\", for_CLA_format=True)\n",
    "# fill_the_templates(tgt_lang_test, tgt_json, languages[1], \"test\", for_CLA_format=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c2a745b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "knowledge_extraction",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
